# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This is **flext-dbt-ldif**, a specialized dbt project for LDIF (LDAP Data Interchange Format) analytics within the FLEXT framework. This project implements advanced data transformations and analytics for LDAP directory data processing.

## Project Architecture

### Programmatically Generated dbt Project

This is a **Python-managed dbt project** rather than a traditional static dbt project:

- **No static dbt_project.yml** - Configuration is managed programmatically
- **No static SQL model files** - Models are generated by Python code in `src/flext_dbt_ldif/`
- **Runtime model generation** - SQL models exist only in compiled form in `target/` directory
- **Sophisticated analytics patterns** - Uses advanced SQL techniques and modern dbt features

### Key Data Models

Based on the compiled manifest, the project contains these models:

#### **Staging Layer (`staging/`)**:
- `stg_ldif_entries` - Core staging model for LDIF data with data quality tests

#### **Intermediate Layer (`intermediate/`)**:
- `int_ldif_analytics` - Intermediate analytics aggregations
- `int_ldif_users` - User-specific data processing

#### **Dimension Layer (`dimensions/`)**:
- `dim_users` - User dimension table

#### **Fact Layer (`facts/`)**:
- `fact_ldif_changes` - Change tracking and impact analysis

#### **Analytics/Marts Layer (`marts/`)**:
- `analytics_ldif_insights` - Advanced analytics with:
  - Time series analysis with window functions
  - Anomaly detection using statistical methods
  - Risk assessment and impact scoring
  - Modern SQL patterns (CTEs, pivots, moving averages)
  - Business intelligence metrics

### Technology Stack

- **dbt 1.10.4** - Data transformation framework
- **PostgreSQL** - Database adapter
- **Python 3.13** - Code generation and management
- **dbt-utils** - Utility macros for pivots and analysis
- **codegen** - Code generation utilities

## Development Commands

### Project Environment

Since this is a programmatically managed project, development commands differ from standard dbt:

```bash
# The project is managed as part of the FLEXT workspace
cd /home/marlonsc/flext/flext-dbt-ldif

# Use parent workspace virtual environment
source /home/marlonsc/flext/.venv/bin/activate

# Standard dbt commands (when models are generated)
dbt deps              # Install dbt packages
dbt compile          # Compile models
dbt run              # Execute models  
dbt test             # Run data quality tests
dbt docs generate    # Generate documentation
dbt docs serve       # Serve documentation
```

### Code Generation Commands

The actual model generation is handled by Python code in `src/flext_dbt_ldif/`:

```bash
# These commands would be project-specific for model generation
# (Commands will depend on the Python implementation in src/)
python -m flext_dbt_ldif.generate_models    # Hypothetical model generation
python -m flext_dbt_ldif.update_schemas     # Hypothetical schema updates
```

## Database Configuration

### Target Database Structure

- **Database**: `ldif_analytics`
- **Schemas**:
  - `ldif_dev` - Development staging models
  - `ldif_dev_marts` - Analytics marts layer
  - `ldif_dev_dbt_test__audit` - dbt test results

### Source Data

- **Source**: `raw.ldif_entries` table
- **Key columns**: `dn` (Distinguished Name), `object_class`
- **Data quality**: Enforced unique and not-null constraints on `dn`

## Advanced Analytics Features

### Modern SQL Patterns Used

1. **Window Functions**: Time series analysis with lag/lead operations
2. **Statistical Analysis**: Standard deviation, percentiles for anomaly detection  
3. **Dynamic Pivots**: Using dbt_utils.pivot for flexible aggregations
4. **CTE Chains**: Complex multi-step transformations
5. **Advanced Aggregations**: Approximate count distinct, moving averages

### Analytics Capabilities

- **Time Series Analysis**: Hourly/daily trend analysis with moving averages
- **Anomaly Detection**: Statistical methods for identifying unusual patterns
- **Risk Assessment**: Multi-dimensional risk scoring and alerting
- **Data Quality Monitoring**: Comprehensive quality metrics and scoring
- **Business Intelligence**: Operational health scores and KPI tracking

## Testing and Quality

### dbt Tests Implemented

- **Source tests**: not_null and unique constraints on critical fields
- **Custom tests**: Business logic validation in `test_ldif_models`
- **Macro tests**: Validation of custom dbt macros in `test_macros`

### Data Quality Framework

- Comprehensive data quality scoring
- Automated anomaly detection
- Risk level classification (HIGH_RISK, SUSPICIOUS, NORMAL)
- Impact scoring for changes

## Custom dbt Features

### Post-Hooks

Models use sophisticated post-hooks for:
- `create_ldif_data_lineage()` - Data lineage tracking
- `optimize_table_performance()` - Performance optimization

### Custom Macros

The project likely includes custom macros for:
- LDIF-specific data transformations
- Advanced analytics calculations
- Data lineage and metadata management

## Development Guidelines

### Working with Generated Models

- **Don't edit compiled SQL** in `target/` directory - these are auto-generated
- **Modify Python generators** in `src/flext_dbt_ldif/` to change model logic
- **Use dbt commands** for testing and execution after generation
- **Check manifest.json** to understand current model structure

### Testing Approach

```bash
# Run data quality tests
dbt test --select source:raw

# Test specific model layers
dbt test --select staging
dbt test --select marts

# Run custom test suites
dbt test --select test_type:custom
```

### Model Dependencies

Models follow this dependency chain:
1. `source.raw.ldif_entries` (raw data)
2. `stg_ldif_entries` (staging with quality checks)
3. `int_*` models (intermediate transformations)
4. `dim_*` and `fact_*` models (dimensional modeling)
5. `analytics_*` models (advanced analytics)

## Important Notes

### Submodule Structure

This project is a git submodule within the larger FLEXT workspace at `/home/marlonsc/flext/`. Any git operations should consider this submodule relationship.

### Python Integration

- **Source code location**: `src/flext_dbt_ldif/`
- **Package structure**: Follows Python package conventions
- **Virtual environment**: Use workspace-level venv at `/home/marlonsc/flext/.venv`

### File Modification Rules

- **Never modify** compiled files in `target/` directory
- **Never modify** dbt artifacts or cache files
- **Always regenerate** models through Python code rather than editing SQL directly
- **Use workspace commands** when available for consistency

### Analytics Performance

The analytics models use sophisticated SQL patterns that may require:
- Adequate database resources for complex window functions
- Proper indexing on time-based columns for time series analysis
- Statistics collection for optimal query planning